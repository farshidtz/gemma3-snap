name: gemma3
base: core24
summary: Gemma 3 inference snap
description: |
  This snap installs an optimized environment for inference with the Gemma 3 Model.

adopt-info: version

grade: devel
confinement: strict

assumes:
  - snapd2.68 # for components support

compression: lzo

environment:
  # Workaround until it gets set by snapd
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION

layout:
  # OpenCL looks in /etc/OpenCL/vendors for .icd files containing the paths to shared objects
  /etc/OpenCL/vendors:
    bind: $SNAP/etc/OpenCL/vendors
  # Intel icd files use an absolute path to Intel OpenCL shared objects. Use a layout to mount the so files at the expected location
  /usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/intel-opencl:
    bind: $SNAP/usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/intel-opencl
  # NVIDIA icd specifies only the shared object name: libnvidia-opencl.so.1
  # libnvidia-opencl.so.1 is inside /snap/stack-utils/x3/usr/lib/x86_64-linux-gnu/, which is already in LD_LIBRARY_PATH

plugs:
  intel-npu:
    interface: custom-device
    custom-device: intel-npu-device
  npu-libs:
    interface: content
    content: npu-libs-2404
    target: $SNAP/npu-libs
  # To allow sideloading models by root
  # home:
  #   read: all
    
hooks:
  install:
    environment:
      # To load libigdrcl.so, for Intel GPU vRAM
      OCL_ICD_VENDORS: $SNAP/etc/OpenCL/vendors
    plugs:
      # For hardware detection
      - hardware-observe
      - kernel-module-observe
      - opengl

parts:
  version:
    plugin: nil
    override-build: |
      craftctl set version="$(git -C $SNAPCRAFT_PROJECT_DIR describe --always --dirty)"

  scripts:
    source: scripts
    plugin: dump
    organize:
      "*": bin/

  engines:
    source: engines
    plugin: dump
    organize:
      "*": engines/

  cli:
    source: https://github.com/canonical/inference-snaps-cli.git
    source-tag: v1.0.0-beta.23
    plugin: go
    build-snaps:
      - go/1.24/stable
    stage-packages:
      - pciutils
      - nvidia-utils # for Nvidia vRAM and Cuda Capability detection
      - clinfo # for Intel GPU vRAM detection
    override-build: |
      craftctl default

      # For tab completion
      ln --symbolic ./modelctl $CRAFT_PART_INSTALL/bin/gemma3
    organize:
      bin/cli: bin/modelctl

  go-chat-client:
    source: https://github.com/jpm-canonical/go-chat-client.git
    source-type: git
    source-tag: v1.0.0-beta.1
    plugin: go
    build-snaps:
      - go/1.24/stable

  common-runtime-dependencies:
    plugin: nil
    stage-snaps:
      - yq # used in server.sh

  components-common:
    plugin: dump
    source: components
    organize:
      "llamacpp/*": (component/llamacpp)
      # Add other components here
    prime:
      # Exclude everything not explicitly organized
      - -*

  llamacpp-license:
    plugin: nil
    source: https://github.com/ggerganov/llama.cpp.git
    source-depth: 1
    override-build: |
      mkdir -p $CRAFT_PART_INSTALL/usr/share/doc/llamacpp/
      cp $CRAFT_PART_SRC/LICENSE $CRAFT_PART_INSTALL/usr/share/doc/llamacpp/
      cp -r $CRAFT_PART_SRC/licenses/* $CRAFT_PART_INSTALL/usr/share/doc/llamacpp/

  llamacpp-amd64v3:
    plugin: dump
    source: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b6886/llamacpp-amd64-avx2.tar.gz
#    source-checksum: sha256/64e9c3f4b3bdfcd9e33e7319d17cd2a30c110bb8e33991413b6f51921cafe845
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llamacpp)

  model-4b-it-q4-0-gguf:
    source: components/model-4b-it-q4-0-gguf
    plugin: dump
    organize:
      "*": (component/model-4b-it-q4-0-gguf)

  mmproj-f16-4b-gguf:
    source: components/mmproj-f16-4b-gguf
    plugin: dump
    organize:
      "*": (component/mmproj-f16-4b-gguf)

components:
  #
  # Engines
  #

  llamacpp:
    type: standard
    summary: llama.cpp Engine using default CPU instruction sets
    description: LLM inference in C/C++
    version: b6884

  # 
  # Models
  #

  model-4b-it-q4-0-gguf:
    type: standard
    summary: Gemma 3 4B instruct QAT Q4_0 GGUF
    description: Quantized model with 4B parameters in gguf format with Q4_0 weight encoding

  mmproj-f16-4b-gguf:
    type: standard
    summary: MMProj for Gemma 3 4B
    description: Multimodal projector for Qwen 2.5 VL 3B

apps:
  gemma3:
    command: bin/gemma3
    completer: bin/completion.bash
    plugs:
      # For hardware detection
      - hardware-observe
      - opengl
      # To access server over network socket
      - network
    environment:
      # To load libigdrcl.so, for Intel GPU vRAM
      OCL_ICD_VENDORS: $SNAP/etc/OpenCL/vendors
      CHAT: $SNAP/bin/go-chat-client

  server:
    command: bin/server.sh
    daemon: simple
    plugs:
      # For inference and hardware detection (via init)
      - opengl
      # Needed by for cuda-uvmfd to listen on a unix socket?
      # Needed for server app by inheritance
      - network-bind
      # Needed to download resources
      - network
      # For hardware detection (via init)
      - hardware-observe
      # For sideloading models
      - home
      # Intel NPU access (device node)
      - intel-npu 
      # Intel NPU access (libs) 
      - npu-libs  
    environment:
      # Needed for shared libraries used by llama.cpp
      # TODO: Should this instead be added in an env file to llamacpp component
      ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR
