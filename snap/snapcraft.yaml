name: gemma3
base: core24
summary: Gemma 3 inference snap
description: |
  This snap installs an optimized environment for inference with the Gemma 3 Model.

adopt-info: version

grade: stable
confinement: strict

assumes:
  - snapd2.68 # for components support

compression: lzo

platforms:
  amd64:
  arm64:

slots:
  status:
    interface: content
    source:
      read:
        - &status-share $SNAP_DATA/share/status

environment:
  # Path to access current revision of components
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION
  # This is to find the intel.icd file, containing the path to libigdrcl.so
  # Required for Intel GPU detection and communication
  OCL_ICD_VENDORS: $SNAP/etc/OpenCL/vendors
  # To find shared libraries that are staged and moved to components
  ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR
  # Status directory shared via content interface
  STATUS_SHARE: *status-share

layout:
  # OpenCL looks in /etc/OpenCL/vendors for .icd files containing the paths to shared objects
  /etc/OpenCL/vendors:
    bind: $SNAP/etc/OpenCL/vendors
  # Intel icd files use an absolute path to Intel OpenCL shared objects. Use a layout to mount the so files at the expected location
  /usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/intel-opencl:
    bind: $SNAP/usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/intel-opencl

plugs:
  intel-npu:
    interface: custom-device
    custom-device: intel-npu-device
  npu-libs:
    interface: content
    content: npu-libs-2404
    target: $SNAP/npu-libs
  # To allow sideloading models by root daemon
  home:
    read: all

hooks:
  install:
    plugs: &install-plugs
      # For hardware detection
      - hardware-observe
      - kernel-module-observe
      - opengl
  post-refresh:
    plugs: *install-plugs

parts:
  version:
    plugin: dump
    source: .
    build-packages:
      - git-lfs
    override-pull: |
      craftctl set version="v3+$(git -C $SNAPCRAFT_PROJECT_DIR describe --always --dirty)"

  scripts:
    source: scripts
    plugin: dump
    organize:
      "*": bin/

  engines:
    source: engines
    plugin: dump
    organize:
      "*": engines/

  cli:
    source: https://github.com/canonical/inference-snaps-cli.git
    source-tag: v1.0.0-beta.28
    plugin: go
    build-snaps:
      - go/1.24/stable
    stage-packages:
      - pciutils
      - nvidia-utils-580 # for Nvidia vRAM and Cuda Capability detection
      - clinfo # for Intel GPU vRAM detection
    override-build: |
      craftctl default

      # For tab completion
      ln --symbolic ./modelctl $CRAFT_PART_INSTALL/bin/gemma3
    organize:
      bin/cli: bin/modelctl
    prime:
      # Exclude nvidia icd to avoid clinfo from looking up properties for nvidia GPUs.
      # See: https://github.com/canonical/inference-snaps/issues/148
      - -etc/OpenCL/vendors/nvidia.icd

  go-chat-client:
    source: https://github.com/jpm-canonical/go-chat-client.git
    source-type: git
    source-tag: v1.0.0-beta.2
    plugin: go
    build-snaps:
      - go/1.24/stable

  common-runtime-dependencies:
    plugin: nil
    stage-snaps:
      - yq # used in server.sh

  component-local-files:
    plugin: dump
    source: components
    organize:
      "llamacpp/*": (component/llamacpp)
      "llamacpp-cuda/*": (component/llamacpp-cuda)
      "mmproj-f16-4b-gguf/*": (component/mmproj-f16-4b-gguf)
      "model-4b-it-int4-fq-ov/*": (component/model-4b-it-int4-fq-ov)
      "model-270m-it-q4-0-gguf/*": (component/model-270m-it-q4-0-gguf)
      "model-4b-it-q4-0-gguf/*": (component/model-4b-it-q4-0-gguf)
      "openvino-model-server/*": (component/openvino-model-server)
      # Add other components here
    prime:
      # Exclude everything not explicitly organized
      - -*

  # Includes the OpenCL ICDs for Intel GPU vRAM lookup
  # libigdgmm is also required for vram lookup to work
  intel-opencl-icd:
    plugin: nil
    build-packages:
      - wget
    override-build: |
      # Intel Compute Runtime is only published for amd64
      if [ "$CRAFT_ARCH_BUILD_FOR" == "amd64" ]; then
        # Legacy
        wget https://github.com/intel/compute-runtime/releases/download/24.35.30872.36/intel-opencl-icd-legacy1_24.35.30872.36_amd64.deb
        wget https://github.com/intel/compute-runtime/releases/download/24.35.30872.36/libigdgmm12_22.5.0_amd64.deb
      
        # Latest
        wget https://github.com/intel/compute-runtime/releases/download/26.01.36711.4/intel-opencl-icd_26.01.36711.4-0_amd64.deb
        wget https://github.com/intel/compute-runtime/releases/download/26.01.36711.4/libigdgmm12_22.9.0_amd64.deb
      
        dpkg --root=$CRAFT_PART_INSTALL --force-all -i *.deb
      
        # Two symlinks are created when installing intel-opencl-icd-legacy: 
        # etc/alternatives/ocloc -> /usr/bin/ocloc-24.35.0, usr/bin/ocloc -> /etc/alternatives/ocloc
        # These point to outside of the snap and fails store review. They are not required, so remove it.
        rm $CRAFT_PART_INSTALL/etc/alternatives/ocloc
        rm $CRAFT_PART_INSTALL/usr/bin/ocloc
      fi
      
      craftctl default

  llamacpp:
    plugin: dump
    source:
      - on amd64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7821/llamacpp-amd64.tar.gz
      - on arm64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7821/llamacpp-arm64.tar.gz
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llamacpp)

  llamacpp-cuda:
    plugin: dump
    source:
      - on amd64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7821/llamacpp-amd64+cuda12.tar.gz
      - on arm64: https://github.com/jpm-canonical/llama.cpp-builds/releases/download/b7821/llamacpp-arm64+cuda12.tar.gz
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llamacpp-cuda)

  openvino-model-server:
    plugin: dump
    source:
      # Source OVMS binaries only on amd64
      - on amd64: https://github.com/openvinotoolkit/model_server/releases/download/v2025.3/ovms_ubuntu24_python_on.tar.gz
      - on arm64: components/openvino-model-server
    stage-packages:
      - on amd64:
          - libxml2
          - curl
          - libpython3.12
    organize:
      # move everything, including the staged packages
      "*": (component/openvino-model-server)

  openvino-model-server-python-dependencies:
    plugin: python
    source: components/openvino-model-server
    python-packages:
      - Jinja2==3.1.6
      - MarkupSafe==3.0.2
    organize:
      "lib": (component/openvino-model-server)
    stage:
      - -*

  # For Intel GPU usage, amd64 only
  intel-compute-runtime:
    plugin: nil
    stage-packages:
      - on amd64:
          - ocl-icd-libopencl1
    override-build: |
      if [ "$CRAFT_ARCH_BUILD_FOR" == "amd64" ]; then
        # First install the legacy support version. Then install the latest, so that latest can override any old libraries.
      
        # Legacy: https://github.com/intel/compute-runtime/releases/tag/24.35.30872.36
        mkdir -p $CRAFT_PART_BUILD/opencl-intel-legacy1
        cd $CRAFT_PART_BUILD/opencl-intel-legacy1
        wget https://github.com/intel/intel-graphics-compiler/releases/download/igc-1.0.17537.24/intel-igc-core_1.0.17537.24_amd64.deb
        wget https://github.com/intel/intel-graphics-compiler/releases/download/igc-1.0.17537.24/intel-igc-opencl_1.0.17537.24_amd64.deb
        wget https://github.com/intel/compute-runtime/releases/download/24.35.30872.36/intel-level-zero-gpu-legacy1_1.5.30872.36_amd64.deb
        dpkg --root=$CRAFT_PART_INSTALL --force-all -i *.deb
      
        # Latest: https://github.com/intel/compute-runtime/releases/tag/26.01.36711.4
        mkdir -p $CRAFT_PART_BUILD/opencl-intel
        cd $CRAFT_PART_BUILD/opencl-intel
        wget https://github.com/intel/intel-graphics-compiler/releases/download/v2.27.10/intel-igc-core-2_2.27.10+20617_amd64.deb
        wget https://github.com/intel/intel-graphics-compiler/releases/download/v2.27.10/intel-igc-opencl-2_2.27.10+20617_amd64.deb
        wget https://github.com/intel/compute-runtime/releases/download/26.01.36711.4/intel-ocloc_26.01.36711.4-0_amd64.deb
        wget https://github.com/intel/compute-runtime/releases/download/26.01.36711.4/libze-intel-gpu1_26.01.36711.4-0_amd64.deb
        dpkg --root=$CRAFT_PART_INSTALL --force-all -i *.deb
      
        cd $CRAFT_PART_BUILD
      fi # End of Intel Compute Runtime installation
      
      craftctl default
    organize:
      # move everything, including the staged packages
      "*": (component/openvino-model-server)


  # Licenses
  # The licenses of staged packages are automatically included in the snap at $SNAP/usr/share/doc/
  # Additional licenses:
  notice:
    plugin: nil
    source: NOTICE
    source-type: file
    override-build: |
      license_dir=$CRAFT_PART_INSTALL/usr/share/doc
      mkdir -p $license_dir
      cp NOTICE $license_dir/
      # Add license files referenced in NOTICE
      cp $SNAPCRAFT_PROJECT_DIR/LICENSE $license_dir/
      cp $SNAPCRAFT_PROJECT_DIR/LICENSE-gemma.md $license_dir/
  llamacpp-license:
    plugin: dump
    source: https://github.com/ggerganov/llama.cpp.git
    source-depth: 1
    override-build: |
      license_dir=$CRAFT_PART_INSTALL/usr/share/doc/llama.cpp
      mkdir -p $license_dir
      cp LICENSE $license_dir/
      cp -r licenses/* $license_dir/
  openvino-model-server-license:
    plugin: dump
    source: https://github.com/openvinotoolkit/model_server.git
    source-depth: 1
    override-build: |
      license_dir=$CRAFT_PART_INSTALL/usr/share/doc/openvino-model-server
      mkdir -p $license_dir
      cp LICENSE $license_dir/  

components:
  #
  # Engines
  #

  llamacpp:
    type: standard
    summary: llama.cpp using default CPU instruction sets
    description: LLM inference in C/C++
    version: b7821

  llamacpp-cuda:
    type: standard
    summary: llama.cpp with CUDA backend
    description: LLM inference in C/C++
    version: b7821


  openvino-model-server:
    type: standard
    summary: OpenVINO Model Server
    description: OpenVINO Model Server for serving models
    version: v2025.3

  # 
  # Models
  #

  model-270m-it-q4-0-gguf:
    type: standard
    summary: Gemma 3 270M instruct Q4_0 GGUF
    description: Quantized model with 270M parameters in gguf format with Q4_0 weight encoding 

  model-4b-it-q4-0-gguf:
    type: standard
    summary: Gemma 3 4B instruct Q4_0 GGUF
    description: Quantized model with 4B parameters in gguf format with Q4_0 weight encoding

  mmproj-f16-4b-gguf:
    type: standard
    summary: MMProj for Gemma 3 4B
    description: Multimodal projector for Qwen 2.5 VL 3B

  model-4b-it-int4-fq-ov:
    type: standard
    summary: Gemma 3 4B instruct INT4 FQ for OpenVINO
    description: |
      This model uses group wise quantization with AWQ and group size 64 for the language model,
      and full static INT8 quantization for vision model

apps:
  gemma3:
    command: bin/gemma3
    completer: bin/completion.bash
    plugs:
      # For hardware detection
      - hardware-observe
      - opengl
      # To access server over network socket
      - network
    environment:
      CHAT: $SNAP/bin/go-chat-client

  server:
    command: bin/server.sh
    daemon: simple
    plugs:
      # For inference and hardware detection
      - opengl
      # Needed to bind on network interfaces
      - network-bind
      # Needed to download resources
      - network
      # For hardware detection
      - hardware-observe
      # For sideloading models
      - home
      # Intel NPU access (device node)
      - intel-npu
      # Intel NPU access (libs) 
      - npu-libs
